{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('eng_songdata.csv', usecols=['text']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Look at her face, it's a wonderful face  \\nAnd...\n",
       "1  Take it easy with me, please  \\nTouch me gentl...\n",
       "2  I'll never know why I had to go  \\nWhy I had t...\n",
       "3  Making somebody happy is a question of give an...\n",
       "4  Making somebody happy is a question of give an..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "apos_end_pattern = r\"'( cause| d| em| ll| m| n| re| s| til| till| twas| ve) (?!')\"\n",
    "apos_start_pattern = r\" (d |j |l |ol |y )'\"\n",
    "apos_double_pattern = r\" ' n ' \"\n",
    "def data_cleaning(dat):\n",
    "    dat.loc[:, 'cleaned_text'] = dat.text.str.lower()\n",
    "    dat.loc[:, 'cleaned_text'] = dat.cleaned_text.str.replace(\"''cause\", \"'cause\")\n",
    "    dat.loc[:, 'cleaned_text'] = dat.cleaned_text.str.replace(\"n't\", \" n't\")\n",
    "    dat.loc[:, 'cleaned_text'] = dat.cleaned_text.str.replace(\"''\", '\"')\n",
    "    dat.loc[:, 'cleaned_text'] = dat.cleaned_text.str.replace(r'[\\n!\"\\(\\),-.0-9:?\\[\\]]', lambda x: ' '+x.group(0)+' ')\n",
    "    dat.loc[:, 'cleaned_text'] = dat.cleaned_text.str.replace(\"'\", \" ' \")\n",
    "    # in' to ing\n",
    "    dat.loc[:, 'cleaned_text'] = dat.cleaned_text.str.replace(r\"\\w+in'$|\\w+in'\\s\", lambda m: m.group(0).replace(\"'\", 'g'))\n",
    "    # recover n't\n",
    "    dat.loc[:, 'cleaned_text'] = dat.cleaned_text.str.replace(r\" n ' t \", \" n't \")\n",
    "    # recover 'cause, 'd, 'em, 'll, 'm, 'n, 're, 's, 'til, 'till, 'twas\n",
    "    dat.loc[:, 'cleaned_text'] = dat.cleaned_text.str.replace(apos_end_pattern, lambda m: m.group(0)[:1]+m.group(0)[2:])\n",
    "    # recover d', j', l', ol', y'\n",
    "    dat.loc[:, 'cleaned_text'] = dat.cleaned_text.str.replace(apos_start_pattern, lambda m: m.group(0)[:-2]+m.group(0)[-1:])\n",
    "    # recover 'n'\n",
    "    dat.loc[:, 'cleaned_text'] = dat.cleaned_text.str.replace(apos_double_pattern, lambda m: m.group(0)[:2]+m.group(0)[3]+m.group(0)[-2:])\n",
    "    dat.loc[:, 'cleaned_text'] = dat.cleaned_text.str.replace(r' {2,}', ' ')\n",
    "    dat.loc[:, 'cleaned_text'] = dat.cleaned_text.str.strip()\n",
    "    dat.loc[:, 'text_list'] = dat.cleaned_text.str.split(' ')\n",
    "    return dat.loc[:, 'text_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = pd.read_table('glove.6B.50d.txt', sep=' ', header=None, quoting=3, na_filter=False, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_words = set(glove.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = data_cleaning(dat.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [look, at, her, face, ,, it, 's, a, wonderful,...\n",
       "1    [take, it, easy, with, me, ,, please, \\n, touc...\n",
       "2    [i, 'll, never, know, why, i, had, to, go, \\n,...\n",
       "3    [making, somebody, happy, is, a, question, of,...\n",
       "4    [making, somebody, happy, is, a, question, of,...\n",
       "Name: text_list, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to remove non-english songs\n",
    "# n_eng = cleaned.apply(lambda x: len(set(x) & glove_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dat.loc[n_eng > 20].to_csv('eng_songdata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sets = cleaned.apply(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_tokens(text_sets):\n",
    "    tokens = set()\n",
    "    for ind, item in text_sets.iteritems():\n",
    "        tokens = tokens | item\n",
    "    return len(tokens), sorted(list(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, unique_tokens = get_unique_tokens(cleaned_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81452"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n', '!', '\"', \"'\", \"'cause\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_unique_tokens(unique_tokens, filename='unique_tokens'):\n",
    "    # check if all unique\n",
    "    assert len(unique_tokens) == len(set(unique_tokens)), 'Make sure all tokens unique!'\n",
    "    unique_token_series = pd.Series(unique_tokens)\n",
    "    unique_token_series.to_csv(filename, index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_unique_tokens(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_word_map(unique_tokens):\n",
    "    # check if all unique\n",
    "    assert len(unique_tokens) == len(set(unique_tokens)), 'Make sure all tokens unique!'\n",
    "    word2ind = {}\n",
    "    ind2word = {}\n",
    "    for ind, word in enumerate(unique_tokens):\n",
    "        word2ind[word] = ind\n",
    "        ind2word[ind] = word\n",
    "    return word2ind, ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2ind, ind2word = get_index_word_map(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(dict2save, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(dict2save, f)\n",
    "\n",
    "def save_index_word_map(word2ind, ind2word, word2ind_filename='word2ind', ind2word_filename='ind2word'):\n",
    "    save_dict(word2ind, word2ind_filename)\n",
    "    save_dict(ind2word, ind2word_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_index_word_map(word2ind, ind2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb(ind2word, imported_emb):\n",
    "    vocab_size = len(ind2word)\n",
    "    n_fact = imported_emb.shape[1]\n",
    "    emb = np.zeros((vocab_size, n_fact))\n",
    "    for i in range(vocab_size):\n",
    "        word = ind2word[i]\n",
    "        try:\n",
    "            emb[i] = imported_emb.loc[word]\n",
    "        except KeyError:\n",
    "            emb[i] = np.random.normal(scale=0.6, size=(n_fact,))\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = create_emb(ind2word, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embedding(embedding, filename='embedding.csv'):\n",
    "    np.savetxt(filename, embedding, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embedding(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_cleaned_data(dat, word2ind):\n",
    "    return dat.apply(lambda words: [word2ind[word] for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised = tokenise_cleaned_data(cleaned, word2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [41434, 3741, 31893, 23851, 19, 36007, 12, 36,...\n",
       "1    [70486, 36007, 21300, 79589, 44029, 19, 54233,...\n",
       "2    [33764, 7, 48341, 38656, 79162, 33764, 30328, ...\n",
       "3    [42765, 66351, 30916, 35884, 36, 56819, 49792,...\n",
       "4    [42765, 66351, 30916, 35884, 36, 56819, 49792,...\n",
       "Name: text_list, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenised.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(text_list, input_length=10):\n",
    "    list_length = len(text_list)\n",
    "    res_list = []\n",
    "    for i in range(0, list_length-input_length):\n",
    "        res_list.append(text_list[i:i+input_length+1])\n",
    "    return res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_samples(pd_series, input_length=10):\n",
    "    transformed_texts = pd_series.apply(transform_text, input_length=input_length)\n",
    "    samples = transformed_texts.sum()\n",
    "    return pd.DataFrame(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = make_training_samples(tokenised, input_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15674922, 21)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41434</td>\n",
       "      <td>3741</td>\n",
       "      <td>31893</td>\n",
       "      <td>23851</td>\n",
       "      <td>19</td>\n",
       "      <td>36007</td>\n",
       "      <td>12</td>\n",
       "      <td>36</td>\n",
       "      <td>79788</td>\n",
       "      <td>23851</td>\n",
       "      <td>...</td>\n",
       "      <td>2348</td>\n",
       "      <td>36007</td>\n",
       "      <td>44070</td>\n",
       "      <td>66380</td>\n",
       "      <td>66904</td>\n",
       "      <td>72665</td>\n",
       "      <td>44029</td>\n",
       "      <td>0</td>\n",
       "      <td>41434</td>\n",
       "      <td>3741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3741</td>\n",
       "      <td>31893</td>\n",
       "      <td>23851</td>\n",
       "      <td>19</td>\n",
       "      <td>36007</td>\n",
       "      <td>12</td>\n",
       "      <td>36</td>\n",
       "      <td>79788</td>\n",
       "      <td>23851</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>36007</td>\n",
       "      <td>44070</td>\n",
       "      <td>66380</td>\n",
       "      <td>66904</td>\n",
       "      <td>72665</td>\n",
       "      <td>44029</td>\n",
       "      <td>0</td>\n",
       "      <td>41434</td>\n",
       "      <td>3741</td>\n",
       "      <td>71714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31893</td>\n",
       "      <td>23851</td>\n",
       "      <td>19</td>\n",
       "      <td>36007</td>\n",
       "      <td>12</td>\n",
       "      <td>36</td>\n",
       "      <td>79788</td>\n",
       "      <td>23851</td>\n",
       "      <td>0</td>\n",
       "      <td>2348</td>\n",
       "      <td>...</td>\n",
       "      <td>44070</td>\n",
       "      <td>66380</td>\n",
       "      <td>66904</td>\n",
       "      <td>72665</td>\n",
       "      <td>44029</td>\n",
       "      <td>0</td>\n",
       "      <td>41434</td>\n",
       "      <td>3741</td>\n",
       "      <td>71714</td>\n",
       "      <td>78316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23851</td>\n",
       "      <td>19</td>\n",
       "      <td>36007</td>\n",
       "      <td>12</td>\n",
       "      <td>36</td>\n",
       "      <td>79788</td>\n",
       "      <td>23851</td>\n",
       "      <td>0</td>\n",
       "      <td>2348</td>\n",
       "      <td>36007</td>\n",
       "      <td>...</td>\n",
       "      <td>66380</td>\n",
       "      <td>66904</td>\n",
       "      <td>72665</td>\n",
       "      <td>44029</td>\n",
       "      <td>0</td>\n",
       "      <td>41434</td>\n",
       "      <td>3741</td>\n",
       "      <td>71714</td>\n",
       "      <td>78316</td>\n",
       "      <td>71689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>36007</td>\n",
       "      <td>12</td>\n",
       "      <td>36</td>\n",
       "      <td>79788</td>\n",
       "      <td>23851</td>\n",
       "      <td>0</td>\n",
       "      <td>2348</td>\n",
       "      <td>36007</td>\n",
       "      <td>44070</td>\n",
       "      <td>...</td>\n",
       "      <td>66904</td>\n",
       "      <td>72665</td>\n",
       "      <td>44029</td>\n",
       "      <td>0</td>\n",
       "      <td>41434</td>\n",
       "      <td>3741</td>\n",
       "      <td>71714</td>\n",
       "      <td>78316</td>\n",
       "      <td>71689</td>\n",
       "      <td>63594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1      2      3      4      5      6      7      8      9   \\\n",
       "0  41434   3741  31893  23851     19  36007     12     36  79788  23851   \n",
       "1   3741  31893  23851     19  36007     12     36  79788  23851      0   \n",
       "2  31893  23851     19  36007     12     36  79788  23851      0   2348   \n",
       "3  23851     19  36007     12     36  79788  23851      0   2348  36007   \n",
       "4     19  36007     12     36  79788  23851      0   2348  36007  44070   \n",
       "\n",
       "   ...       11     12     13     14     15     16     17     18     19     20  \n",
       "0  ...     2348  36007  44070  66380  66904  72665  44029      0  41434   3741  \n",
       "1  ...    36007  44070  66380  66904  72665  44029      0  41434   3741  71714  \n",
       "2  ...    44070  66380  66904  72665  44029      0  41434   3741  71714  78316  \n",
       "3  ...    66380  66904  72665  44029      0  41434   3741  71714  78316  71689  \n",
       "4  ...    66904  72665  44029      0  41434   3741  71714  78316  71689  63594  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_samples(samples, filename='train.csv'):\n",
    "    samples.to_csv('train.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_training_samples(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
