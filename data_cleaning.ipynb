{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('songdata.csv', usecols=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Look at her face, it's a wonderful face  \\nAnd...\n",
       "1  Take it easy with me, please  \\nTouch me gentl...\n",
       "2  I'll never know why I had to go  \\nWhy I had t...\n",
       "3  Making somebody happy is a question of give an...\n",
       "4  Making somebody happy is a question of give an..."
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "apos_end_pattern = r\"'( cause| d| em| ll| m| n| re| s| til| till| twas| ve) (?!')\"\n",
    "apos_start_pattern = r\" (d |j |l |ol |y )'\"\n",
    "apos_double_pattern = r\" ' n ' \"\n",
    "def data_cleaning(dat):\n",
    "    dat.loc[:, 'cleaned_text'] = dat.text.str.lower()\n",
    "    dat.loc[:, 'cleaned_text'] = dat.cleaned_text.str.replace(r'[\\n!\"\\(\\),-.0-9:?\\[\\]]', lambda x: ' '+x.group(0)+' ')\n",
    "    dat.loc[:, 'cleaned_text'] = dat.cleaned_text.str.replace(\"''\", '\"')\n",
    "    dat.loc[:, 'cleaned_text'] = dat.cleaned_text.str.replace(\"'\", \" ' \")\n",
    "    # in' to ing\n",
    "    dat.loc[:, 'cleaned_text'] = dat.cleaned_text.str.replace(r\"\\w+in'$|\\w+in'\\s\", lambda m: m.group(0).replace(\"'\", 'g'))\n",
    "    # recover 'cause, 'd, 'em, 'll, 'm, 'n, 're, 's, 'til, 'till, 'twas\n",
    "    dat.loc[:, 'cleaned_text'] = dat.cleaned_text.str.replace(apos_end_pattern, lambda m: m.group(0)[:1]+m.group(0)[2:])\n",
    "    # recover d', j', l', ol', y'\n",
    "    dat.loc[:, 'cleaned_text'] = dat.cleaned_text.str.replace(apos_start_pattern, lambda m: m.group(0)[:-2]+m.group(0)[-1:])\n",
    "    # recover 'n'\n",
    "    dat.loc[:, 'cleaned_text'] = dat.cleaned_text.str.replace(apos_double_pattern, lambda m: m.group(0)[:2]+m.group(0)[3]+m.group(0)[-2:])\n",
    "    dat.loc[:, 'cleaned_text'] = dat.cleaned_text.str.replace(r' {2,}', ' ')\n",
    "    dat.loc[:, 'cleaned_text'] = dat.cleaned_text.str.strip()\n",
    "    dat.loc[:, 'text_list'] = dat.cleaned_text.str.split(' ')\n",
    "    return dat.loc[:, 'text_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = data_cleaning(dat.head(2000).copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [look, at, her, face, ,, it, 's, a, wonderful,...\n",
       "1    [take, it, easy, with, me, ,, please, \\n, touc...\n",
       "2    [i, 'll, never, know, why, i, had, to, go, \\n,...\n",
       "3    [making, somebody, happy, is, a, question, of,...\n",
       "4    [making, somebody, happy, is, a, question, of,...\n",
       "Name: text_list, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_tokens(tokens):\n",
    "    unique_tokens = sorted(list(set(tokens)))\n",
    "    return len(unique_tokens), unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = cleaned.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, unique_tokens = get_unique_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13063"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_unique_tokens(unique_tokens, filename='unique_tokens'):\n",
    "    # check if all unique\n",
    "    assert len(unique_tokens) == len(set(unique_tokens)), 'Make sure all tokens unique!'\n",
    "    unique_token_series = pd.Series(unique_tokens)\n",
    "    unique_token_series.to_csv(filename, index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_unique_tokens(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_word_map(unique_tokens):\n",
    "    # check if all unique\n",
    "    assert len(unique_tokens) == len(set(unique_tokens)), 'Make sure all tokens unique!'\n",
    "    word2ind = {}\n",
    "    ind2word = {}\n",
    "    for ind, word in enumerate(unique_tokens):\n",
    "        word2ind[word] = ind\n",
    "        ind2word[ind] = word\n",
    "    return word2ind, ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2ind, ind2word = get_index_word_map(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(dict2save, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(dict2save, f)\n",
    "\n",
    "def save_index_word_map(word2ind, ind2word, word2ind_filename='word2ind', ind2word_filename='ind2word'):\n",
    "    save_dict(word2ind, word2ind_filename)\n",
    "    save_dict(ind2word, ind2word_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_index_word_map(word2ind, ind2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = pd.read_table('glove.6B.50d.txt', sep=' ', header=None, quoting=3, na_filter=False, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb(ind2word, imported_emb):\n",
    "    vocab_size = len(ind2word)\n",
    "    n_fact = imported_emb.shape[1]\n",
    "    emb = np.zeros((vocab_size, n_fact))\n",
    "    for i in range(vocab_size):\n",
    "        word = ind2word[i]\n",
    "        try:\n",
    "            emb[i] = imported_emb.loc[word]\n",
    "        except KeyError:\n",
    "            emb[i] = np.random.normal(scale=0.6, size=(n_fact,))\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = create_emb(ind2word, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embedding(embedding, filename='embedding.csv'):\n",
    "    np.savetxt(filename, embedding, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embedding(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_cleaned_data(dat, word2ind):\n",
    "    return dat.apply(lambda words: [word2ind[word] for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised = tokenise_cleaned_data(cleaned, word2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [6615, 586, 5250, 3882, 21, 5866, 14, 38, 1285...\n",
       "1    [11363, 5866, 3515, 12826, 7004, 21, 8515, 0, ...\n",
       "2    [5552, 9, 7609, 6197, 12739, 5552, 5013, 11731...\n",
       "3    [6831, 10578, 5089, 5848, 38, 8960, 7801, 4686...\n",
       "4    [6831, 10578, 5089, 5848, 38, 8960, 7801, 4686...\n",
       "Name: text_list, dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenised.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(text_list, input_length=10):\n",
    "    list_length = len(text_list)\n",
    "    res_list = []\n",
    "    for i in range(0, list_length-input_length):\n",
    "        res_list.append(text_list[i:i+input_length+1])\n",
    "    return res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_samples(pd_series):\n",
    "    transformed_texts = pd_series.apply(transform_text)\n",
    "    samples = transformed_texts.sum()\n",
    "    return pd.DataFrame(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = make_training_samples(tokenised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(555801, 11)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6615</td>\n",
       "      <td>586</td>\n",
       "      <td>5250</td>\n",
       "      <td>3882</td>\n",
       "      <td>21</td>\n",
       "      <td>5866</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "      <td>12859</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>586</td>\n",
       "      <td>5250</td>\n",
       "      <td>3882</td>\n",
       "      <td>21</td>\n",
       "      <td>5866</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "      <td>12859</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5250</td>\n",
       "      <td>3882</td>\n",
       "      <td>21</td>\n",
       "      <td>5866</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "      <td>12859</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>386</td>\n",
       "      <td>5866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3882</td>\n",
       "      <td>21</td>\n",
       "      <td>5866</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "      <td>12859</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>386</td>\n",
       "      <td>5866</td>\n",
       "      <td>7018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>5866</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "      <td>12859</td>\n",
       "      <td>3882</td>\n",
       "      <td>0</td>\n",
       "      <td>386</td>\n",
       "      <td>5866</td>\n",
       "      <td>7018</td>\n",
       "      <td>10587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3      4      5      6      7      8     9      10\n",
       "0  6615   586  5250  3882     21   5866     14     38  12859  3882      0\n",
       "1   586  5250  3882    21   5866     14     38  12859   3882     0    386\n",
       "2  5250  3882    21  5866     14     38  12859   3882      0   386   5866\n",
       "3  3882    21  5866    14     38  12859   3882      0    386  5866   7018\n",
       "4    21  5866    14    38  12859   3882      0    386   5866  7018  10587"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_samples(samples, filename='train.csv'):\n",
    "    samples.to_csv('train.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_training_samples(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
