{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model, Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model20180120_7.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_index_word_map(word2ind_filename='word2ind', ind2word_filename='ind2word'):\n",
    "    word2ind = load_dict(word2ind_filename)\n",
    "    ind2word = load_dict(ind2word_filename)\n",
    "    return word2ind, ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2ind, ind2word = load_index_word_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "apos_end_pattern = r\"'( cause| d| em| ll| m| n| re| s| til| till| twas| ve) (?!')\"\n",
    "apos_start_pattern = r\" (d |j |l |ol |y )'\"\n",
    "apos_double_pattern = r\" ' n ' \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_cleaning(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace(\"''cause\", \"'cause\")\n",
    "    sentence = sentence.replace(\"n't\", \" n't\")\n",
    "    sentence = sentence.replace(\"''\", '\"')\n",
    "    sentence = re.sub(r'[\\n!\"\\(\\),-.0-9:?\\[\\]]', lambda x: ' '+x.group(0)+' ', sentence)\n",
    "    sentence = re.sub(r\"\\w+in'$|\\w+in'\\s\", lambda m: m.group(0).replace(\"'\", 'g'), sentence)\n",
    "    sentence = sentence.replace(\"'\", \" ' \")\n",
    "    # recover 'cause, 'd, 'em, 'll, 'm, 'n, 're, 's, 'til, 'till, 'twas\n",
    "    sentence = re.sub(apos_end_pattern, lambda m: m.group(0)[:1]+m.group(0)[2:], sentence)\n",
    "    # recover d', j', l', ol', y'\n",
    "    sentence = re.sub(apos_start_pattern, lambda m: m.group(0)[:-2]+m.group(0)[-1:], sentence)\n",
    "    # recover 'n'\n",
    "    sentence = re.sub(apos_double_pattern, lambda m: m.group(0)[:2]+m.group(0)[3]+m.group(0)[-2:], sentence)\n",
    "    sentence = sentence.replace(\" n ' t \", \" n't \")\n",
    "    sentence = re.sub(r' {2,}', ' ', sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = sentence.split(' ')\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(words):\n",
    "    return [word2ind.get(word, 0) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenise(tokens):\n",
    "    return [ind2word[str(ind)] for ind in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unapos_end_pattern = r\" '(d|em|ll|m|n|re|s|til|till|twas|ve) \"\n",
    "unapos_start_pattern = r\" (d|j|l|ol|y)' \"\n",
    "unapos_double_pattern = r\" 'n' \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting(sentence):\n",
    "    sentence = re.sub(r'[\\(\\[] ', lambda x: x.group(0)[:-1], sentence)\n",
    "    sentence = re.sub(r' [\\)\\].!?,:]', lambda x: x.group(0)[1:], sentence)\n",
    "    sentence = re.sub(r'\" .+ \"', lambda x: x.group(0).replace('\" ', '\"').replace(' \"', '\"'), sentence)\n",
    "    sentence = re.sub(r'^[\\[\\(\"]?\\w|[?.!]\"? \\w', lambda x: x.group(0).upper(), sentence)\n",
    "    sentence = re.sub(r' i ', ' I ', sentence)\n",
    "    sentence = re.sub(unapos_start_pattern, lambda x: x.group(0)[:-1], sentence)\n",
    "    sentence = re.sub(unapos_end_pattern, lambda x: x.group(0)[1:], sentence)\n",
    "    sentence = re.sub(unapos_double_pattern, lambda x: x.group(0)[1:-1], sentence)\n",
    "    sentence = sentence.replace(\" n't \", \"n't \")\n",
    "    # doin ' => doin'\n",
    "    sentence = re.sub(r\"(\\win) (' )\", lambda m: m.group(1)+m.group(2), sentence)\n",
    "    sentence = re.sub(r' {2,}', ' ', sentence)\n",
    "    # first letter alignment\n",
    "    sentence = re.sub(r'(\\n) (\\w)', lambda m: m.group(1)+m.group(2).upper(), sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_not_end(tokens):\n",
    "    unequal_square_quotes = tokens.count(word2ind['[']) != tokens.count(word2ind[']'])\n",
    "    unequal_brackets = tokens.count(word2ind['(']) != tokens.count(word2ind[')'])\n",
    "    odd_double_quotes = tokens.count(word2ind['\"']) % 2 != 0\n",
    "    nonfinished_sentence = ind2word[str(tokens[-1])] not in ['.', '!', '?', '\"', '\\n']\n",
    "    not_end = unequal_square_quotes or unequal_brackets or odd_double_quotes or nonfinished_sentence\n",
    "    return not_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement(seed_text, n_words, maxlen=10, must_stop=200):\n",
    "    cleaned = input_cleaning(seed_text)\n",
    "    padded_input_tokens = tokenise(cleaned)\n",
    "    res_tokens = [token for token in padded_input_tokens]\n",
    "    while (n_words > 0 or semantic_not_end(padded_input_tokens)) and must_stop > 0:\n",
    "        padded_input_tokens = pad_sequences([padded_input_tokens], maxlen=maxlen)\n",
    "        probs = model.predict(padded_input_tokens)[0]\n",
    "        probs = probs / np.sum(probs)\n",
    "        predicted = np.random.choice(list(range(len(probs))), p=probs)\n",
    "        padded_input_tokens = padded_input_tokens[0].tolist()\n",
    "        padded_input_tokens.append(predicted)\n",
    "        res_tokens.append(predicted)\n",
    "        n_words -= 1\n",
    "        must_stop -= 1\n",
    "    detokenised = detokenise(res_tokens)\n",
    "    return formatting(' '.join(detokenised))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace seed_text with your own\n",
    "res = implement(seed_text=\"I can't sleep as she said she liked me\", n_words=500, maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't sleep as she said she liked me that ole surprise \n",
      "I found his brother like a flower bound and you walking high \n",
      " \n",
      "Fighting and the squirrel came by the soul \n",
      "Sometimes now it's too late \n",
      "The sweet way they won't lose us away \n",
      "Expecting townsend this is there \n",
      " \n",
      "Life is alright, give up the cry \n",
      "We know that he worries us around \n",
      "We let it out, we'll take it \n",
      "We can't sing it trip \n",
      "We be listening in the middle of need \n",
      "No better song \n",
      "Don't hold on and stop it just, I don't need no best baby \n",
      " 'cause she's goin' up one time, trying to be right \n",
      "But I need to touch, lockets she now I'm sleeping \n",
      "You got me, needs, scream. Under an love star. \n",
      "Baby, I want everything to me, yeah, \n",
      "Oh, oh, oh, when you're there \n",
      "And when I go to your mind, oh that's the way \n",
      "Was we found a call?\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
