{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model20180126_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_index_word_map(word2ind_filename='word2ind', ind2word_filename='ind2word'):\n",
    "    word2ind = load_dict(word2ind_filename)\n",
    "    ind2word = load_dict(ind2word_filename)\n",
    "    return word2ind, ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2ind, ind2word = load_index_word_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ending = [word2ind['('], word2ind['end'], word2ind[')']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 22216, 18]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "apos_end_pattern = r\"'( cause| d| em| ll| m| n| re| s| til| till| twas| ve) (?!')\"\n",
    "apos_start_pattern = r\" (d |j |l |ol |y )'\"\n",
    "apos_double_pattern = r\" ' n ' \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_cleaning(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace(\"''cause\", \"'cause\")\n",
    "    sentence = sentence.replace(\"n't\", \" n't\")\n",
    "    sentence = sentence.replace(\"''\", '\"')\n",
    "    sentence = re.sub(r'[\\n!\"\\(\\),-.0-9:?\\[\\]]', lambda x: ' '+x.group(0)+' ', sentence)\n",
    "    sentence = re.sub(r\"\\w+in'$|\\w+in'\\s\", lambda m: m.group(0).replace(\"'\", 'g'), sentence)\n",
    "    sentence = sentence.replace(\"'\", \" ' \")\n",
    "    # recover 'cause, 'd, 'em, 'll, 'm, 'n, 're, 's, 'til, 'till, 'twas\n",
    "    sentence = re.sub(apos_end_pattern, lambda m: m.group(0)[:1]+m.group(0)[2:], sentence)\n",
    "    # recover d', j', l', ol', y'\n",
    "    sentence = re.sub(apos_start_pattern, lambda m: m.group(0)[:-2]+m.group(0)[-1:], sentence)\n",
    "    # recover 'n'\n",
    "    sentence = re.sub(apos_double_pattern, lambda m: m.group(0)[:2]+m.group(0)[3]+m.group(0)[-2:], sentence)\n",
    "    sentence = sentence.replace(\" n ' t \", \" n't \")\n",
    "    sentence = re.sub(r' {2,}', ' ', sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = sentence.split(' ')\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(words):\n",
    "    return [word2ind.get(word, 0) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenise(tokens):\n",
    "    return [ind2word[str(ind)] for ind in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unapos_end_pattern = r\" '(d|em|ll|m|n|re|s|til|till|twas|ve) \"\n",
    "unapos_start_pattern = r\" (d|j|l|ol|y)' \"\n",
    "unapos_double_pattern = r\" 'n' \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting(sentence):\n",
    "    sentence = re.sub(r'[\\(\\[] ', lambda x: x.group(0)[:-1], sentence)\n",
    "    sentence = re.sub(r' [\\)\\].!?,:]', lambda x: x.group(0)[1:], sentence)\n",
    "    sentence = re.sub(r'\" .+ \"', lambda x: x.group(0).replace('\" ', '\"').replace(' \"', '\"'), sentence)\n",
    "    sentence = re.sub(r'^[\\[\\(\"]?\\w|[?.!]\"? \\w', lambda x: x.group(0).upper(), sentence)\n",
    "    sentence = re.sub(r' i ', ' I ', sentence)\n",
    "    sentence = re.sub(unapos_start_pattern, lambda x: x.group(0)[:-1], sentence)\n",
    "    sentence = re.sub(unapos_end_pattern, lambda x: x.group(0)[1:], sentence)\n",
    "    sentence = re.sub(unapos_double_pattern, lambda x: x.group(0)[1:-1], sentence)\n",
    "    sentence = sentence.replace(\" n't \", \"n't \")\n",
    "    # doin ' => doin'\n",
    "    sentence = re.sub(r\"(\\win) (' )\", lambda m: m.group(1)+m.group(2), sentence)\n",
    "    sentence = re.sub(r' {2,}', ' ', sentence)\n",
    "    # first letter alignment\n",
    "    sentence = re.sub(r'(\\n) (\\w)', lambda m: m.group(1)+m.group(2).upper(), sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_not_end(tokens):\n",
    "    unequal_square_quotes = tokens.count(word2ind['[']) != tokens.count(word2ind[']'])\n",
    "    unequal_brackets = tokens.count(word2ind['(']) != tokens.count(word2ind[')'])\n",
    "    odd_double_quotes = tokens.count(word2ind['\"']) % 2 != 0\n",
    "    nonfinished_sentence = ind2word[str(tokens[-1])] not in ['.', '!', '?', '\"', '\\n']\n",
    "    not_end = unequal_square_quotes or unequal_brackets or odd_double_quotes or nonfinished_sentence\n",
    "    return not_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement(seed_text, n_words, maxlen=10, must_stop=2000):\n",
    "    cleaned = input_cleaning(seed_text)\n",
    "    padded_input_tokens = tokenise(cleaned)\n",
    "    res_tokens = [token for token in padded_input_tokens]\n",
    "    last_three = deque([0, 0, 0], maxlen=3)\n",
    "    while (n_words > 0 or semantic_not_end(padded_input_tokens)) and must_stop > 0 and (last_three[0] != ending[0] or last_three[1] != ending[1] or last_three[2] != ending[2]):\n",
    "        padded_input_tokens = pad_sequences([padded_input_tokens], maxlen=maxlen)\n",
    "        probs = model.predict(padded_input_tokens)[0]\n",
    "        probs = probs / np.sum(probs)\n",
    "        predicted = np.random.choice(list(range(len(probs))), p=probs)\n",
    "        padded_input_tokens = padded_input_tokens[0].tolist()\n",
    "        padded_input_tokens.append(predicted)\n",
    "        res_tokens.append(predicted)\n",
    "        n_words -= 1\n",
    "        must_stop -= 1\n",
    "        last_three.append(predicted)\n",
    "    detokenised = detokenise(res_tokens)\n",
    "    return formatting(' '.join(detokenised))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace seed_text with your own\n",
    "res = implement(seed_text=\"I can't sleep as she said she liked me\", n_words=500, maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't sleep as she said she liked me \n",
      "So hard ain't no less \n",
      "So why, one more story \n",
      " \n",
      "I knew I said that I see me \n",
      " \n",
      "For two forty - seconds of the water \n",
      "The walls will stay home \n",
      "Lord was no mobile, and no \n",
      " \n",
      "Since the wheel can be a celebration \n",
      "Even let me see my hearts down. \n",
      "When you see, they said \n",
      "Never see me for yesterday \n",
      "A bore is gone and grow eyes \n",
      "Somebody could like you who wont hear my time \n",
      "No laugh alone now \n",
      "Nor I've found one in trouble \n",
      "You were in christmas nights \n",
      "This makes a friend \n",
      "They'll happy farther there \n",
      "That is the only soul cause I want (be never waiting, to let you touch your love) \n",
      "Time inside this night and the world is standing \n",
      " \n",
      "I'll be there, new, you'll let you be blind \n",
      "You better many of your life, who forever my love \n",
      "All your decision is meant to say \n",
      "But what can you fake if you worry \n",
      "You had some voice, no human love, baby \n",
      "God, every christmas's chemicals I need my back alive \n",
      " \n",
      " (end)\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
