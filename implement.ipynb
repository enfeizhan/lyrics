{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model, Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model20180120_7.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_index_word_map(word2ind_filename='word2ind', ind2word_filename='ind2word'):\n",
    "    word2ind = load_dict(word2ind_filename)\n",
    "    ind2word = load_dict(ind2word_filename)\n",
    "    return word2ind, ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2ind, ind2word = load_index_word_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "apos_end_pattern = r\"'( cause| d| em| ll| m| n| re| s| til| till| twas| ve) (?!')\"\n",
    "apos_start_pattern = r\" (d |j |l |ol |y )'\"\n",
    "apos_double_pattern = r\" ' n ' \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_cleaning(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace(\"''cause\", \"'cause\")\n",
    "    sentence = sentence.replace(\"n't\", \" n't\")\n",
    "    sentence = sentence.replace(\"''\", '\"')\n",
    "    sentence = re.sub(r'[\\n!\"\\(\\),-.0-9:?\\[\\]]', lambda x: ' '+x.group(0)+' ', sentence)\n",
    "    sentence = re.sub(r\"\\w+in'$|\\w+in'\\s\", lambda m: m.group(0).replace(\"'\", 'g'), sentence)\n",
    "    sentence = sentence.replace(\"'\", \" ' \")\n",
    "    # recover 'cause, 'd, 'em, 'll, 'm, 'n, 're, 's, 'til, 'till, 'twas\n",
    "    sentence = re.sub(apos_end_pattern, lambda m: m.group(0)[:1]+m.group(0)[2:], sentence)\n",
    "    # recover d', j', l', ol', y'\n",
    "    sentence = re.sub(apos_start_pattern, lambda m: m.group(0)[:-2]+m.group(0)[-1:], sentence)\n",
    "    # recover 'n'\n",
    "    sentence = re.sub(apos_double_pattern, lambda m: m.group(0)[:2]+m.group(0)[3]+m.group(0)[-2:], sentence)\n",
    "    sentence = sentence.replace(\" n ' t \", \" n't \")\n",
    "    sentence = re.sub(r' {2,}', ' ', sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = sentence.split(' ')\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(words):\n",
    "    return [word2ind.get(word, 0) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenise(tokens):\n",
    "    return [ind2word[str(ind)] for ind in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unapos_end_pattern = r\" '(d|em|ll|m|n|re|s|til|till|twas|ve) \"\n",
    "unapos_start_pattern = r\" (d|j|l|ol|y)' \"\n",
    "unapos_double_pattern = r\" 'n' \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting(sentence):\n",
    "    sentence = re.sub(r'[\\(\\[] ', lambda x: x.group(0)[:-1], sentence)\n",
    "    sentence = re.sub(r' [\\)\\].!?,:]', lambda x: x.group(0)[1:], sentence)\n",
    "    sentence = re.sub(r'\" .+ \"', lambda x: x.group(0).replace('\" ', '\"').replace(' \"', '\"'), sentence)\n",
    "    sentence = re.sub(r'^[\\[\\(\"]?\\w|[?.!]\"? \\w', lambda x: x.group(0).upper(), sentence)\n",
    "    sentence = re.sub(r' i ', ' I ', sentence)\n",
    "    sentence = re.sub(unapos_start_pattern, lambda x: x.group(0)[:-1], sentence)\n",
    "    sentence = re.sub(unapos_end_pattern, lambda x: x.group(0)[1:], sentence)\n",
    "    sentence = re.sub(unapos_double_pattern, lambda x: x.group(0)[1:-1], sentence)\n",
    "    sentence = sentence.replace(\" n't \", \"n't \")\n",
    "    # doin ' => doin'\n",
    "    sentence = re.sub(r\"(\\win) (' )\", lambda m: m.group(1)+m.group(2), sentence)\n",
    "    sentence = re.sub(r' {2,}', ' ', sentence)\n",
    "    # first letter alignment\n",
    "    sentence = re.sub(r'(\\n) (\\w)', lambda m: m.group(1)+m.group(2).upper(), sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_not_end(tokens):\n",
    "    unequal_square_quotes = tokens.count(word2ind['[']) != tokens.count(word2ind[']'])\n",
    "    unequal_brackets = tokens.count(word2ind['(']) != tokens.count(word2ind[')'])\n",
    "    odd_double_quotes = tokens.count(word2ind['\"']) % 2 != 0\n",
    "    nonfinished_sentence = ind2word[str(tokens[-1])] not in ['.', '!', '?', '\"', '\\n']\n",
    "    not_end = unequal_square_quotes or unequal_brackets or odd_double_quotes or nonfinished_sentence\n",
    "    return not_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement(seed_text, n_words, maxlen=10, must_stop=200):\n",
    "    cleaned = input_cleaning(seed_text)\n",
    "    padded_input_tokens = tokenise(cleaned)\n",
    "    res_tokens = [token for token in padded_input_tokens]\n",
    "    while (n_words > 0 or semantic_not_end(padded_input_tokens)) and must_stop > 0:\n",
    "        padded_input_tokens = pad_sequences([padded_input_tokens], maxlen=maxlen)\n",
    "        probs = model.predict(padded_input_tokens)[0]\n",
    "        predicted = np.random.choice(list(range(len(probs))), p=probs)\n",
    "        padded_input_tokens = padded_input_tokens[0].tolist()\n",
    "        padded_input_tokens.append(predicted)\n",
    "        res_tokens.append(predicted)\n",
    "        n_words -= 1\n",
    "        must_stop -= 1\n",
    "    detokenised = detokenise(res_tokens)\n",
    "    return formatting(' '.join(detokenised))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace seed_text with your own\n",
    "res = implement(seed_text=\"I can't sleep as she said she liked me\", n_words=500, maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't sleep as she said she liked me \n",
      "Good time and you opened out me fifteen miles \n",
      " \n",
      "You'd probably tell me what to do, let me go steppin' \n",
      "But a drag's rising low, oh where long grows? \n",
      " \n",
      "How I can't love that where I'm blessed from nightmare to bury the sidewalk \n",
      "Everyone is a thousand - yeah \n",
      "Maybe he's gonna squeeze him, little woman takes your hands low \n",
      "Mama'll be listening, maybe she's hurry one more line \n",
      "I won't stop nothin' I want, hug a friend \n",
      " \n",
      "Always failing \n",
      "Still with you \n",
      " \n",
      "No and I'm the last thing I have left there \n",
      "I'm moving back to myself for the test of men \n",
      "She's brushing your ride \n",
      "They say I had that song \n",
      "I - I won't give me no magic to come \n",
      "You're sleeping by the journey, no one hears'em \n",
      "He'll lay the stone and obey us. \n",
      " \n",
      "The president I just rather know is breaking me by the rain.\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
