{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model, Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model20171227_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 13062\n",
    "emb_dim = 50\n",
    "input_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=emb_dim, input_length=input_length))\n",
    "model.add(LSTM(1000, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(1000, activation='relu', return_sequences=True))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model20171227.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_index_word_map(word2ind_filename='word2ind', ind2word_filename='ind2word'):\n",
    "    word2ind = load_dict(word2ind_filename)\n",
    "    ind2word = load_dict(ind2word_filename)\n",
    "    return word2ind, ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2ind, ind2word = load_index_word_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "apos_end_pattern = r\"'( cause| d| em| ll| m| n| re| s| til| till| twas| ve) (?!')\"\n",
    "apos_start_pattern = r\" (d |j |l |ol |y )'\"\n",
    "apos_double_pattern = r\" ' n ' \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_cleaning(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[\\n!\"\\(\\),-.0-9:?\\[\\]]', lambda x: ' '+x.group(0)+' ', sentence)\n",
    "    sentence = sentence.replace(\"''\", '\"')\n",
    "    sentence = re.sub(r\"\\w+in'$|\\w+in'\\s\", lambda m: m.group(0).replace(\"'\", 'g'), sentence)\n",
    "    sentence = sentence.replace(\"'\", \" ' \")\n",
    "    # recover 'cause, 'd, 'em, 'll, 'm, 'n, 're, 's, 'til, 'till, 'twas\n",
    "    sentence = re.sub(apos_end_pattern, lambda m: m.group(0)[:1]+m.group(0)[2:], sentence)\n",
    "    # recover d', j', l', ol', y'\n",
    "    sentence = re.sub(apos_start_pattern, lambda m: m.group(0)[:-2]+m.group(0)[-1:], sentence)\n",
    "    # recover 'n'\n",
    "    sentence = re.sub(apos_double_pattern, lambda m: m.group(0)[:2]+m.group(0)[3]+m.group(0)[-2:], sentence)\n",
    "    sentence = re.sub(r' {2,}', ' ', sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = sentence.split(' ')\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(words):\n",
    "    return [word2ind.get(word, 0) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenise(tokens):\n",
    "    return [ind2word[str(ind)] for ind in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unapos_end_pattern = r\" '(d|em|ll|m|n|re|s|til|till|twas|ve) \"\n",
    "unapos_start_pattern = r\" (d|j|l|ol|y)' \"\n",
    "unapos_double_pattern = r\" 'n' \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncleaning(sentence):\n",
    "    sentence = re.sub(r'[\\(\\[] ', lambda x: x.group(0)[:-1], sentence)\n",
    "    sentence = re.sub(r' [\\)\\].!?]', lambda x: x.group(0)[1:], sentence)\n",
    "    sentence = re.sub(r'\" .+ \"', lambda x: x.group(0).replace('\" ', '\"').replace(' \"', '\"'), sentence)\n",
    "    sentence = re.sub(r'^[\\[\\(\"]?\\w|[?.!]\"? \\w', lambda x: x.group(0).upper(), sentence)\n",
    "    sentence = re.sub(r' i ', ' I ', sentence)\n",
    "    sentence = re.sub(unapos_start_pattern, lambda x: x.group(0)[:-1], sentence)\n",
    "    sentence = re.sub(unapos_end_pattern, lambda x: x.group(0)[1:], sentence)\n",
    "    sentence = re.sub(unapos_double_pattern, lambda x: x.group(0)[1:-1], sentence)\n",
    "    sentence = re.sub(r' {2,}', ' ', sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_not_end(tokens):\n",
    "    unequal_square_quotes = tokens.count(word2ind['[']) != tokens.count(word2ind[']'])\n",
    "    unequal_brackets = tokens.count(word2ind['(']) != tokens.count(word2ind[')'])\n",
    "    odd_double_quotes = tokens.count(word2ind['\"']) % 2 != 0\n",
    "    nonfinished_sentence = ind2word[str(tokens[-1])] not in ['.', '!', '?', '\"']\n",
    "    not_end = unequal_square_quotes or unequal_brackets or odd_double_quotes or nonfinished_sentence\n",
    "    return not_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement(seed_text, n_words, maxlen=10, must_stop=200):\n",
    "    cleaned = input_cleaning(seed_text)\n",
    "    res_tokens = padded_input_tokens = tokenise(cleaned)\n",
    "    while (n_words > 0 or semantic_not_end(padded_input_tokens)) and must_stop > 0:\n",
    "        padded_input_tokens = pad_sequences([padded_input_tokens], maxlen=maxlen)\n",
    "        predicted = np.argmax(model.predict(padded_input_tokens))\n",
    "        padded_input_tokens = padded_input_tokens[0].tolist()\n",
    "        padded_input_tokens.append(predicted)\n",
    "        res_tokens.append(predicted)\n",
    "        n_words -= 1\n",
    "        must_stop -= 1\n",
    "    detokenised = detokenise(res_tokens)\n",
    "    return uncleaning(' '.join(detokenised))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = implement('standing in the rain, enjoying the moment in the dark', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standing in the rain , enjoying the moment in the dark , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
