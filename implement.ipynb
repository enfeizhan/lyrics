{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model, Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model20180117_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_index_word_map(word2ind_filename='word2ind', ind2word_filename='ind2word'):\n",
    "    word2ind = load_dict(word2ind_filename)\n",
    "    ind2word = load_dict(ind2word_filename)\n",
    "    return word2ind, ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2ind, ind2word = load_index_word_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "apos_end_pattern = r\"'( cause| d| em| ll| m| n| re| s| til| till| twas| ve) (?!')\"\n",
    "apos_start_pattern = r\" (d |j |l |ol |y )'\"\n",
    "apos_double_pattern = r\" ' n ' \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_cleaning(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace(\"''cause\", \"'cause\")\n",
    "    sentence = sentence.replace(\"n't\", \" n't\")\n",
    "    sentence = sentence.replace(\"''\", '\"')\n",
    "    sentence = re.sub(r'[\\n!\"\\(\\),-.0-9:?\\[\\]]', lambda x: ' '+x.group(0)+' ', sentence)\n",
    "    sentence = re.sub(r\"\\w+in'$|\\w+in'\\s\", lambda m: m.group(0).replace(\"'\", 'g'), sentence)\n",
    "    sentence = sentence.replace(\"'\", \" ' \")\n",
    "    # recover 'cause, 'd, 'em, 'll, 'm, 'n, 're, 's, 'til, 'till, 'twas\n",
    "    sentence = re.sub(apos_end_pattern, lambda m: m.group(0)[:1]+m.group(0)[2:], sentence)\n",
    "    # recover d', j', l', ol', y'\n",
    "    sentence = re.sub(apos_start_pattern, lambda m: m.group(0)[:-2]+m.group(0)[-1:], sentence)\n",
    "    # recover 'n'\n",
    "    sentence = re.sub(apos_double_pattern, lambda m: m.group(0)[:2]+m.group(0)[3]+m.group(0)[-2:], sentence)\n",
    "    sentence = sentence.replace(\" n ' t \", \" n't \")\n",
    "    sentence = re.sub(r' {2,}', ' ', sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = sentence.split(' ')\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(words):\n",
    "    return [word2ind.get(word, 0) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenise(tokens):\n",
    "    return [ind2word[str(ind)] for ind in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unapos_end_pattern = r\" '(d|em|ll|m|n|re|s|til|till|twas|ve) \"\n",
    "unapos_start_pattern = r\" (d|j|l|ol|y)' \"\n",
    "unapos_double_pattern = r\" 'n' \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting(sentence):\n",
    "    sentence = re.sub(r'[\\(\\[] ', lambda x: x.group(0)[:-1], sentence)\n",
    "    sentence = re.sub(r' [\\)\\].!?,:]', lambda x: x.group(0)[1:], sentence)\n",
    "    sentence = re.sub(r'\" .+ \"', lambda x: x.group(0).replace('\" ', '\"').replace(' \"', '\"'), sentence)\n",
    "    sentence = re.sub(r'^[\\[\\(\"]?\\w|[?.!]\"? \\w', lambda x: x.group(0).upper(), sentence)\n",
    "    sentence = re.sub(r' i ', ' I ', sentence)\n",
    "    sentence = re.sub(unapos_start_pattern, lambda x: x.group(0)[:-1], sentence)\n",
    "    sentence = re.sub(unapos_end_pattern, lambda x: x.group(0)[1:], sentence)\n",
    "    sentence = re.sub(unapos_double_pattern, lambda x: x.group(0)[1:-1], sentence)\n",
    "    sentence = sentence.replace(\" n't \", \"n't \")\n",
    "    # doin ' => doin'\n",
    "    sentence = re.sub(r\"(\\win) (' )\", lambda m: m.group(1)+m.group(2), sentence)\n",
    "    sentence = re.sub(r' {2,}', ' ', sentence)\n",
    "    # first letter alignment\n",
    "    sentence = re.sub(r'(\\n) (\\w)', lambda m: m.group(1)+m.group(2).upper(), sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_not_end(tokens):\n",
    "    unequal_square_quotes = tokens.count(word2ind['[']) != tokens.count(word2ind[']'])\n",
    "    unequal_brackets = tokens.count(word2ind['(']) != tokens.count(word2ind[')'])\n",
    "    odd_double_quotes = tokens.count(word2ind['\"']) % 2 != 0\n",
    "    nonfinished_sentence = ind2word[str(tokens[-1])] not in ['.', '!', '?', '\"', '\\n']\n",
    "    not_end = unequal_square_quotes or unequal_brackets or odd_double_quotes or nonfinished_sentence\n",
    "    return not_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement(seed_text, n_words, maxlen=10, must_stop=200):\n",
    "    cleaned = input_cleaning(seed_text)\n",
    "    padded_input_tokens = tokenise(cleaned)\n",
    "    res_tokens = [token for token in padded_input_tokens]\n",
    "    while (n_words > 0 or semantic_not_end(padded_input_tokens)) and must_stop > 0:\n",
    "        padded_input_tokens = pad_sequences([padded_input_tokens], maxlen=maxlen)\n",
    "        probs = model.predict(padded_input_tokens)[0]\n",
    "        predicted = np.random.choice(list(range(len(probs))), p=probs)\n",
    "        padded_input_tokens = padded_input_tokens[0].tolist()\n",
    "        padded_input_tokens.append(predicted)\n",
    "        res_tokens.append(predicted)\n",
    "        n_words -= 1\n",
    "        must_stop -= 1\n",
    "    detokenised = detokenise(res_tokens)\n",
    "    return formatting(' '.join(detokenised))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace seed_text with your own\n",
    "res = implement(seed_text=\"I can't sleep as she said she liked me\", n_words=500, maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't sleep as she said she liked me alive to you \n",
      "I love you and I've been asking so long for me baby this love just \n",
      " \n",
      "I will be dreaming of how I'm alright \n",
      "When I go under the street \n",
      "Afraid of the sun and stones \n",
      "Oh the lord eve in the park \n",
      " \n",
      "All \n",
      "For love is time \n",
      "Your sadness is gonna go \n",
      "And I'm a scandal of you \n",
      "Here we're hanging in the farm \n",
      " \n",
      "Take me tomorrow \n",
      "And say that it's near \n",
      "Baby help it when I fall back breaking you under that side \n",
      "In your own chances again \n",
      "There he'll mend the dark \n",
      "But it's only harder \n",
      "And all there brings \n",
      "Time in sea I don't feel impossible \n",
      " \n",
      "You are on my sleeve, I gotta make myself good \n",
      "Because I want some one that's a bite of \n",
      "Play, lame we gotta celebrate \n",
      "Was you comin' home \n",
      "Hiding for a cottage top around the sun fallin' outside \n",
      "There was as high as long as far\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't sleep as she said she liked me \n",
      "I'm in bed and tell you that wasn't giving me a call \n",
      "What him can't get it for yourself ' - dig \n",
      " \n",
      "The birds and marian are the closing \n",
      "In my mind and then run \n",
      "And break the spirit in the land \n",
      " (repeat chorus) \n",
      "They will see the think of the crown in the sword \n",
      "The hell leaves their hearts no we wait... \n",
      "Ain't nothin' I'm not on my side \n",
      "So to die (good night) \n",
      "You got someone else \n",
      "That man can't find me to mouth? \n",
      "I'm leaving the girl that I'm feeling around \n",
      " \n",
      "I've found the world of love and I can't come nowhere to make what I do \n",
      "Now sound right home to me \n",
      "I've seen a way that you're there \n",
      " 'cause my name gets I got \n",
      " \n",
      "I knew that the key didn't end in our saddle \n",
      "On the jeep \n",
      "It's going right lazy \n",
      "And even love you in miami\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't sleep as she said she liked me \n",
      "Goodbye and she said \n",
      " \n",
      "What for us now \n",
      "It used to wait so falling \n",
      "Once \n",
      "I want to stop up \n",
      "There's the truth someday \n",
      "You have to buy a sense \n",
      "Break me up 's, far round my heart \n",
      "Laugh on the roses on the carousel \n",
      "You check apart, we came for me \n",
      "You're so long and harm \n",
      " \n",
      "Learn to play the captain \n",
      "I remember something I grew you \n",
      "Stars's haunting blame \n",
      "Straight I've been nothing when you can say \n",
      "Someone made all you'll play \n",
      " \n",
      "The darkness that eternal of the pain \n",
      "Jesus, tell a deeper appear! \n",
      " (i see, cold) \n",
      "And when the finest lights is only flowing above \n",
      "I last like a cadillacs people were games \n",
      "1 0 0 - the boy blues at the wings \n",
      "T - la - da - s - hm - huh -, \" grade maids \n",
      " \n",
      "Heard, da - la - l, got ya hotter \n",
      "I, tv sweet!\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
